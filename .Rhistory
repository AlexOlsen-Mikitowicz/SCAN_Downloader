geom_boxplot()+
xlab("Sleep Medication Trial")+
ylab("Length of Additional Sleep Time")+
ggtitle("Comparison of Sleep Medication Treatments")
q2_pio <- .08
q2_pi <- 18/150
q2_teststat <- (q2_pi-q2_pio)/(sqrt((q2_pio)*(1-q2_pio)/150))
q2_z <- qnorm(.95)
q2_teststat
q2_z
#test if ok to use large sample approximation
3*sqrt((q2_pi*(1-q2_pi))/150)
1-q2_pi
#another way to double check test
prop.test(18, 150, p = .08, alternative = "greater", correct = FALSE)
library(car)
library(emmeans)
eggs_wide <- read.csv('cuckoo.csv')
str(eggs_wide)
eggs <- gather(eggs_wide,"HostSpecies","eggsize",na.rm=TRUE) #missing values stacked
eggs <- eggs %>%
as_tibble() %>%
mutate_if(is.character, as.factor)
str(eggs)
scan_downloader = function(scan_site) {
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2") %>%
length(scan_test) = length(site_data)
return(site_data)
}
all_scan_sites <- map_dfr(scan_test, scan_downloader)
scan_downloader = function(scan_site) {
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2") %>%
return(site_data)
}
all_scan_sites <- map_dfr(scan_test, scan_downloader)
library(tidyverse)
library(lubridate)
library(purrr)
library(dplyr)
#you need to install packages for the functions listed in the library call before this will work
library(riem)
#pulls automated surface observations stations - commonly found at airports and is limited to weather data
library(RNRCS)
#Downloads Natural Resources Conservation Service (NRCS) data for sites in the Soil Climate
# Analysis Network (SCAN) <https://www.wcc.nrcs.usda.gov/scan/>, and Snow Telemetry (SNOTEL and SNOLITE)
# <https://www.wcc.nrcs.usda.gov/snow/> networks. Metadata can be returned for all sites in the NRCS' Air and
# Water Data Base (AWDB) <https://www.wcc.nrcs.usda.gov/report_generator/AWDB_Network_Codes.pdf>.
library(metScanR)
#A tool for locating, mapping, and gathering environmental data and metadata, worldwide.
# Users can search for and filter metadata from > 157,000 environmental monitoring stations among 219
# countries/territories and >20 networks/organizations via elevation, location, active dates, elements
# measured (e.g., temperature, precipitation), country, network, and/or known identifier. Future updates to the
# package will allow the user to obtain datasets from stations within the database
scan_sites <- getId(id="SCAN")
scan_sites1 <- grabNRCS.meta("SCAN")
scan_sites_tibble <- as_tibble(scan_sites1)
scan_sites2 <- do.call(what = rbind, args = scan_sites1)
scan_id <- scan_sites2 %>%
select(site_id) %>%
separate(., col = site_id, into = c("Network", "ID"), sep = ":") %>%
select(ID)
scan_test <- scan_id %>%
slice(1:5) %>%
as_vector()
scan_id = scan_sites2 %>%
select(site_id) %>%
separate(., col = site_id, into = c("Network", "ID"), sep = ":") %>%
select(ID)
scan_id <- scan_sites2 %>%
select(site_id) %>%
separate(., col = site_id, into = c("Network", "ID"), sep = ":") %>%
select(ID)
scan_id <- scan_sites2 %>%
select(site_id) %>%
separate(., col = site_id, into = c("Network", "ID"), sep = ":") %>%
select(ID)
scan_data <- grabNRCS.data("SCAN", 2229, "daily", "2019-01-1", "2019-01-10")
scan_downloader = function(scan_site) {
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2") %>%
length(scan_test) = length(site_data) %>%
cbind(scan_test, site_data)
return(site_data)
}
all_scan_sites <- map_dfr(scan_test, scan_downloader)
library(installr)
updateR()
scan_id <- scan_sites2 %>%
select(site_id) %>%
separate(., col = site_id, into = c("Network", "ID"), sep = ":") %>%
select(ID)
install.packages("tidyverse")
library(tidyverse)
library(lubridate)
library(purrr)
library(dplyr)
scan_id <- scan_sites2 %>%
select(site_id) %>%
separate(., col = site_id, into = c("Network", "ID"), sep = ":") %>%
select(ID)
library(installr)
library(tidyverse)
library(lubridate)
library(purrr)
library(dplyr)
#you need to install packages for the functions listed in the library call before this will work
library(riem)
#pulls automated surface observations stations - commonly found at airports and is limited to weather data
library(RNRCS)
#Downloads Natural Resources Conservation Service (NRCS) data for sites in the Soil Climate
# Analysis Network (SCAN) <https://www.wcc.nrcs.usda.gov/scan/>, and Snow Telemetry (SNOTEL and SNOLITE)
# <https://www.wcc.nrcs.usda.gov/snow/> networks. Metadata can be returned for all sites in the NRCS' Air and
# Water Data Base (AWDB) <https://www.wcc.nrcs.usda.gov/report_generator/AWDB_Network_Codes.pdf>.
library(metScanR)
#A tool for locating, mapping, and gathering environmental data and metadata, worldwide.
# Users can search for and filter metadata from > 157,000 environmental monitoring stations among 219
# countries/territories and >20 networks/organizations via elevation, location, active dates, elements
# measured (e.g., temperature, precipitation), country, network, and/or known identifier. Future updates to the
# package will allow the user to obtain datasets from stations within the database
scan_sites <- getId(id="SCAN")
scan_sites1 <- grabNRCS.meta("SCAN")
scan_sites_tibble <- as_tibble(scan_sites1)
scan_sites2 <- do.call(what = rbind, args = scan_sites1)
scan_id <- scan_sites2 %>%
select(site_id) %>%
separate(., col = site_id, into = c("Network", "ID"), sep = ":") %>%
select(ID)
scan_test <- scan_id %>%
slice(1:5) %>%
as_vector()
scan_downloader = function(scan_site) {
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2") %>%
length(scan_test) = length(site_data) %>%
cbind(scan_test, site_data)
return(site_data)
}
all_scan_sites <- map_dfr(scan_test, scan_downloader)
test_scan_download <- grabNRCS.data(network = "SCAN", site_id = 2229, timescale = "daily", DayBgn = "2019-01-1", DayEnd = "2019-01-2")
cbind_test <- test_scan_download
length(scan_test) = length(test_scan_download)
cbind(scan_test, test_scan_download)
str(test_scan_download)
scan_data <- grabNRCS.data("SCAN", 2229, "daily", "2019-01-1", "2019-01-10")
#example of how to pull scan data from a single site
#if you include this function grabNRCS.data in a loop you will be able to download a list of sites
#let me know if you need help with this part because I have done it before
scan_sites <- getId(id="SCAN")
scan_sites1 <- grabNRCS.meta("SCAN")
scan_sites_tibble <- as_tibble(scan_sites1)
scan_sites2 <- do.call(what = rbind, args = scan_sites1)
scan_id <- scan_sites2 %>%
select(site_id) %>%
separate(., col = site_id, into = c("Network", "ID"), sep = ":") %>%
select(ID)
scan_id <- scan_sites2 %>%
select(site_id) %>%
separate(., col = site_id, into = c("Network", "ID"), sep = ":") %>%
select(ID)
library(tidyverse)
library(lubridate)
library(purrr)
library(dplyr)
#you need to install packages for the functions listed in the library call before this will work
library(riem)
#pulls automated surface observations stations - commonly found at airports and is limited to weather data
library(RNRCS)
#Downloads Natural Resources Conservation Service (NRCS) data for sites in the Soil Climate
# Analysis Network (SCAN) <https://www.wcc.nrcs.usda.gov/scan/>, and Snow Telemetry (SNOTEL and SNOLITE)
# <https://www.wcc.nrcs.usda.gov/snow/> networks. Metadata can be returned for all sites in the NRCS' Air and
# Water Data Base (AWDB) <https://www.wcc.nrcs.usda.gov/report_generator/AWDB_Network_Codes.pdf>.
library(metScanR)
#A tool for locating, mapping, and gathering environmental data and metadata, worldwide.
# Users can search for and filter metadata from > 157,000 environmental monitoring stations among 219
# countries/territories and >20 networks/organizations via elevation, location, active dates, elements
# measured (e.g., temperature, precipitation), country, network, and/or known identifier. Future updates to the
# package will allow the user to obtain datasets from stations within the database
scan_sites <- getId(id="SCAN")
scan_sites1 <- grabNRCS.meta("SCAN")
scan_sites_tibble <- as_tibble(scan_sites1)
scan_sites2 <- do.call(what = rbind, args = scan_sites1)
scan_id <- scan_sites2 %>%
select(site_id) %>%
separate(., col = site_id, into = c("Network", "ID"), sep = ":") %>%
select(ID)
scan_test <- scan_id %>%
slice(1:5) %>%
as_vector()
scan_downloader = function(scan_site) {
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2") %>%
length(scan_test) = length(site_data) %>%
cbind(scan_test, site_data)
return(site_data)
}
test_scan_download <- grabNRCS.data(network = "SCAN", site_id = 2229, timescale = "daily", DayBgn = "2019-01-1", DayEnd = "2019-01-2")
cbind_test <- test_scan_download %>%
length(scan_test) = length(test_scan_download)
cbind_test <- test_scan_download
scan_test <- scan_id %>%
slice(1:1) %>%
as_vector()
View(cbind_test)
scan_data <- grabNRCS.data("SCAN", 2229, "daily", "2019-01-1", "2019-01-10")
scan_test <- scan_id %>%
slice(2:5) %>%
as_vector()
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2") %>%
cbind(scan_test, site_data)
scan_downloader = function(scan_site) {
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2") %>%
cbind(scan_test, site_data)
return(site_data)
}
all_scan_sites <- map_dfr(scan_test, scan_downloader)
scan_downloader = function(scan_site) {
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2")
return(site_data)
}
all_scan_sites <- map_dfr(scan_test, scan_downloader)
View(all_scan_sites)
scan_test <- scan_id %>%
slice(1:1) %>%
as_vector()
cbind_trial <- test_scan_download %>%
cbind(., scan_test)
cbind_trial <- test_scan_download %>%
cbind(scan_test, test_scan_download)
cbind_trial <- test_scan_download %>%
cbind.data.frame(scan_test, test_scan_download)
cbind_trial <- test_scan_download %>%
length(scan_test)=length(test_scan_download) %>%
cbind.data.frame(scan_test, test_scan_download)
cbind_trial <- test_scan_download %>%
length(scan_test) <- length(test_scan_download) %>%
cbind.data.frame(scan_test, test_scan_download)
cbind_trial <- test_scan_download %>%
length(scan_test) <- length(test_scan_download) %>%
cbind(scan_test, test_scan_download)
cbind_trial <- test_scan_download %>%
length(scan_test) <- length(test_scan_download) %>%
merge(scan_test, test_scan_download)
cbind_trial <- test_scan_download %>%
length(scan_test) <- length(test_scan_download)
cbind_trial <- test_scan_download %>%
length(scan_test) <- length(test_scan_download)
cbind_trial <- test_scan_download %>%
length(scan_test) = length(test_scan_download)
cbind_trial <- test_scan_download %>%
length(scan_test) = length(test_scan_download)
View(scan_id)
View(scan_sites_tibble)
View(scan_sites1)
cbind_test <- scan_test %>%
length(cbind_test) = length(test_scan_download) %>%
cbind(., test_scan_download)
scan_test <- scan_id %>%
slice(1:2) %>%
as_vector()
cbind_test <- scan_test %>%
length(cbind_test) = length(test_scan_download) %>%
cbind(., test_scan_download)
cbind_test <- scan_test %>%
length(cbind_test) = length(test_scan_download)
cbind_test <- scan_test %>%
length(cbind_test) <- length(test_scan_download)
cbind_test <- scan_test %>%
length(cbind_test) = length(test_scan_download)
install.packages("gdata")
library(gdata)
cbind_test <- scan_test %>%
cbindX(., test_scan_download)
cbind_test <- as.matrix(scan_test)
cbind_test <- as.matrix(scan_test)
cbind_test <- as.matrix(scan_test) %>%
cbindX(., test_scan_download)
scan_test <- scan_id %>%
slice(1:10) %>%
as_vector()
scan_downloader = function(scan_site) {
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2")
return(site_data)
}
all_scan_sites <- map_dfr(scan_test, scan_downloader)
scan_downloader = function(scan_site) {
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2") %>%
mutate(SCAN_ID = site_id)
return(site_data)
}
all_scan_sites <- map_dfr(scan_test, scan_downloader)
scan_downloader = function(scan_site) {
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2") %>%
mutate(SCAN_ID = scan_site)
return(site_data)
}
all_scan_sites <- map_dfr(scan_test, scan_downloader)
View(scan_id)
scan_test <- scan_id %>%
slice(1:30) %>%
as_vector()
scan_downloader = function(scan_site) {
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2") %>%
mutate(SCAN_ID = scan_site)
return(site_data)
}
all_scan_sites <- map_dfr(scan_test, scan_downloader)
View(all_scan_sites)
library(tidyverse)
library(lubridate)
library(purrr)
library(dplyr)
library(gdata)
#you need to install packages for the functions listed in the library call before this will work
library(riem)
#pulls automated surface observations stations - commonly found at airports and is limited to weather data
library(RNRCS)
#Downloads Natural Resources Conservation Service (NRCS) data for sites in the Soil Climate
# Analysis Network (SCAN) <https://www.wcc.nrcs.usda.gov/scan/>, and Snow Telemetry (SNOTEL and SNOLITE)
# <https://www.wcc.nrcs.usda.gov/snow/> networks. Metadata can be returned for all sites in the NRCS' Air and
# Water Data Base (AWDB) <https://www.wcc.nrcs.usda.gov/report_generator/AWDB_Network_Codes.pdf>.
library(metScanR)
#A tool for locating, mapping, and gathering environmental data and metadata, worldwide.
# Users can search for and filter metadata from > 157,000 environmental monitoring stations among 219
# countries/territories and >20 networks/organizations via elevation, location, active dates, elements
# measured (e.g., temperature, precipitation), country, network, and/or known identifier. Future updates to the
# package will allow the user to obtain datasets from stations within the database
scan_sites <- getId(id="SCAN")
scan_sites1 <- grabNRCS.meta("SCAN")
scan_sites_tibble <- as_tibble(scan_sites1)
scan_sites2 <- do.call(what = rbind, args = scan_sites1)
scan_id <- scan_sites2 %>%
select(site_id) %>%
separate(., col = site_id, into = c("Network", "ID"), sep = ":") %>%
select(ID)
#can be used to subset the scan stations, change the slice(x:y), then replace scan_id in line 64 with scan_test
scan_test <- scan_id %>%
slice(1:5) %>%
as_vector()
#function for downloading all scan data. If you want to change the date range change it below (ymd format). If you want to change the time step change "daily" to "hourly" for example.
scan_downloader = function(scan_site) {
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2") %>%
mutate(SCAN_ID = scan_site)
return(site_data)
}
#maps or iterates the function defined above with each site id defined in scan_id dataframe
all_scan_sites <- map_dfr(scan_test, scan_downloader)
#example script for downloading from one SCAN station
#test_scan_download <- grabNRCS.data(network = "SCAN", site_id = 2229, timescale = "daily", DayBgn = "2019-01-1", DayEnd = "2019-01-2")
View(all_scan_sites)
View(scan_sites_tibble)
library(tidyverse)
library(lubridate)
library(purrr)
library(dplyr)
#you need to install packages for the functions listed in the library call before this will work
#library(riem)
#pulls automated surface observations stations - commonly found at airports and is limited to weather data
library(RNRCS)
#Downloads Natural Resources Conservation Service (NRCS) data for sites in the Soil Climate
# Analysis Network (SCAN) <https://www.wcc.nrcs.usda.gov/scan/>, and Snow Telemetry (SNOTEL and SNOLITE)
# <https://www.wcc.nrcs.usda.gov/snow/> networks. Metadata can be returned for all sites in the NRCS' Air and
# Water Data Base (AWDB) <https://www.wcc.nrcs.usda.gov/report_generator/AWDB_Network_Codes.pdf>.
library(metScanR)
#A tool for locating, mapping, and gathering environmental data and metadata, worldwide.
# Users can search for and filter metadata from > 157,000 environmental monitoring stations among 219
# countries/territories and >20 networks/organizations via elevation, location, active dates, elements
# measured (e.g., temperature, precipitation), country, network, and/or known identifier. Future updates to the
# package will allow the user to obtain datasets from stations within the database
scan_sites <- getId(id="SCAN")
scan_sites1 <- grabNRCS.meta("SCAN")
#scan_sites_tibble <- as_tibble(scan_sites1)
scan_sites2 <- do.call(what = rbind, args = scan_sites1)
scan_id <- scan_sites2 %>%
select(site_id) %>%
separate(., col = site_id, into = c("Network", "ID"), sep = ":") %>%
select(ID)
#can be used to subset the scan stations, change the slice(x:y), then replace scan_id in line 64 with scan_test
scan_test <- scan_id %>%
slice(1:5) %>%
as_vector()
#function for downloading all scan data. If you want to change the date range change it below (ymd format). If you want to change the time step change "daily" to "hourly" for example.
scan_downloader = function(scan_site) {
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2") %>%
mutate(SCAN_ID = scan_site)
return(site_data)
}
#maps or iterates the function defined above with each site id defined in scan_id dataframe
all_scan_sites <- map_dfr(scan_test, scan_downloader)
#example script for downloading from one SCAN station
#test_scan_download <- grabNRCS.data(network = "SCAN", site_id = 2229, timescale = "daily", DayBgn = "2019-01-1", DayEnd = "2019-01-2")
View(all_scan_sites)
View(scan_sites1)
View(scan_sites2)
?write_csv()
setwd("P:/Classes/Misc_Proj/SCAN_Downloader")
?write_csv()
tmp <- tempfile()
write_csv(all_scan_sites, tmp)
dir<-tempdir()
dir<-tempdir()
write_csv(all_scan_sites, file.path(dir, "scan_sites.csv"))
dir <- getwd()
dir <- getwd()
write_csv(all_scan_sites, file.path(dir, "scan_sites.csv"))
setwd(dirname(current_path ))
setwd()
getwd()
View(scan_sites2)
library(tidyverse)
library(lubridate)
library(purrr)
library(dplyr)
#you need to install packages for the functions listed in the library call before this will work
#library(riem)
#pulls automated surface observations stations - commonly found at airports and is limited to weather data
library(RNRCS)
#Downloads Natural Resources Conservation Service (NRCS) data for sites in the Soil Climate
# Analysis Network (SCAN) <https://www.wcc.nrcs.usda.gov/scan/>, and Snow Telemetry (SNOTEL and SNOLITE)
# <https://www.wcc.nrcs.usda.gov/snow/> networks. Metadata can be returned for all sites in the NRCS' Air and
# Water Data Base (AWDB) <https://www.wcc.nrcs.usda.gov/report_generator/AWDB_Network_Codes.pdf>.
library(metScanR)
#A tool for locating, mapping, and gathering environmental data and metadata, worldwide.
# Users can search for and filter metadata from > 157,000 environmental monitoring stations among 219
# countries/territories and >20 networks/organizations via elevation, location, active dates, elements
# measured (e.g., temperature, precipitation), country, network, and/or known identifier. Future updates to the
# package will allow the user to obtain datasets from stations within the database
getwd()
scan_sites <- getId(id="SCAN")
scan_sites1 <- grabNRCS.meta("SCAN")
scan_sites2 <- do.call(what = rbind, args = scan_sites1)
scan_id <- scan_sites2 %>%
select(site_id) %>%
separate(., col = site_id, into = c("Network", "ID"), sep = ":") %>%
select(ID)
#can be used to subset the scan stations, change the slice(x:y), then replace scan_id in line 64 with scan_test
scan_test <- scan_id %>%
slice(1:5) %>%
as_vector()
#function for downloading all scan data. If you want to change the date range change it below (ymd format). If you want to change the time step change "daily" to "hourly" for example.
scan_downloader = function(scan_site) {
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2") %>%
mutate(SCAN_ID = scan_site)
return(site_data)
}
#sets the directory to your current working directory
dir <- getwd()
#use below code if you want to set your own directory, uncomment line below and delete or uncomment above line
#dir <- "paste your desired working directory here"
write_csv(all_scan_sites, file.path(dir, "scan_sites.csv"))
#maps or iterates the function defined above with each site id defined in scan_id dataframe
#all_scan_sites <- map_dfr(scan_test, scan_downloader)
#example script for downloading from one SCAN station
#test_scan_download <- grabNRCS.data(network = "SCAN", site_id = 2229, timescale = "daily", DayBgn = "2019-01-1", DayEnd = "2019-01-2")
scan_sites <- getId(id="SCAN")
scan_sites1 <- grabNRCS.meta("SCAN")
scan_sites2 <- do.call(what = rbind, args = scan_sites1)
scan_id <- scan_sites2 %>%
select(site_id) %>%
separate(., col = site_id, into = c("Network", "ID"), sep = ":") %>%
select(ID)
#can be used to subset the scan stations, change the slice(x:y), then replace scan_id in line 64 with scan_test
scan_test <- scan_id %>%
slice(1:5) %>%
as_vector()
#function for downloading all scan data. If you want to change the date range change it below (ymd format). If you want to change the time step change "daily" to "hourly" for example.
scan_downloader = function(scan_site) {
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2") %>%
mutate(SCAN_ID = scan_site)
return(site_data)
}
library(tidyverse)
library(lubridate)
library(purrr)
library(dplyr)
#you need to install packages for the functions listed in the library call before this will work
#library(riem)
#pulls automated surface observations stations - commonly found at airports and is limited to weather data
library(RNRCS)
#Downloads Natural Resources Conservation Service (NRCS) data for sites in the Soil Climate
# Analysis Network (SCAN) <https://www.wcc.nrcs.usda.gov/scan/>, and Snow Telemetry (SNOTEL and SNOLITE)
# <https://www.wcc.nrcs.usda.gov/snow/> networks. Metadata can be returned for all sites in the NRCS' Air and
# Water Data Base (AWDB) <https://www.wcc.nrcs.usda.gov/report_generator/AWDB_Network_Codes.pdf>.
library(metScanR)
#A tool for locating, mapping, and gathering environmental data and metadata, worldwide.
# Users can search for and filter metadata from > 157,000 environmental monitoring stations among 219
# countries/territories and >20 networks/organizations via elevation, location, active dates, elements
# measured (e.g., temperature, precipitation), country, network, and/or known identifier. Future updates to the
# package will allow the user to obtain datasets from stations within the database
getwd()
scan_sites <- getId(id="SCAN")
scan_sites1 <- grabNRCS.meta("SCAN")
scan_sites2 <- do.call(what = rbind, args = scan_sites1)
scan_id <- scan_sites2 %>%
select(site_id) %>%
separate(., col = site_id, into = c("Network", "ID"), sep = ":") %>%
select(ID)
#can be used to subset the scan stations, change the slice(x:y), then replace scan_id in line 64 with scan_test
scan_test <- scan_id %>%
slice(1:3) %>%
as_vector()
#function for downloading all scan data. If you want to change the date range change it below (ymd format). If you want to change the time step change "daily" to "hourly" for example.
scan_downloader = function(scan_site) {
site_data = grabNRCS.data(network = "SCAN", site_id = scan_site, timescale = "daily", "2019-01-1", "2019-01-2") %>%
mutate(SCAN_ID = scan_site)
return(site_data)
}
#maps or iterates the function defined above with each site id defined in scan_id dataframe
all_scan_sites <- map_dfr(scan_test, scan_downloader)
#sets the directory to your current working directory
dir <- getwd()
#use below code if you want to set your own directory, uncomment line below and delete or uncomment above line
#dir <- "paste your desired working directory here"
write_csv(all_scan_sites, file.path(dir, "scan_sites.csv"))
#example script for downloading from one SCAN station
#test_scan_download <- grabNRCS.data(network = "SCAN", site_id = 2229, timescale = "daily", DayBgn = "2019-01-1", DayEnd = "2019-01-2")
